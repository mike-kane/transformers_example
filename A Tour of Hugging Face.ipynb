{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Quick Tour of Hugging Face\n",
    "\n",
    "The goal of this notebook is to act as a \"quick start\" guide to help everyone get up and running with BERT (and all the other open-source Transformer models) using Hugging Face's `transformers` library!\n",
    "\n",
    "## What is this Library?\n",
    "\n",
    "Hugging Face's `transformers` package is a comprehensive open-source (Apache 2.0 License) library that makes it easy to use over 30 different cutting-edge transformer-based models, such as BERT and GPT-2. The library supports both TensorFlow 2 and PyTorch, and has a simple, Keras-like interface. Retraining and fine-tuning models typically only takes a few lines of code, and Hugging Face has created great [documentation](https://github.com/huggingface/transformers) that includes starter code and tutorials.  \n",
    "\n",
    "## Installation\n",
    "\n",
    "To run the sample code in this notebook, you'll first need to install the transformers library. To do so, please run the the cell below, or see the Hugging Face's [full installation guide](https://github.com/huggingface/transformers#installation). \n",
    "\n",
    "**_NOTE:_** We strongly recommend using a virtual environment! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip install tensorflow_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1: Sequence Classification\n",
    "\n",
    "In the following cell, we:\n",
    "\n",
    "**1.** Import everything from the `transformers` library, as well as `tensorflow` and the `tensorflow_datasets` library. \n",
    "<br>\n",
    "<br>\n",
    "**2.** Next, we load a BERT tokenizer. Remember, different models have different tokenizers! The model we'll be using is `'bert-base-cased'`.  For a full list of available models, see [this list](https://github.com/huggingface/transformers#quick-tour).\n",
    "<br>\n",
    "<br>\n",
    "**3.** Similarly, we load the model. Just like the tokenizer, we just pass in a string that references the model we want. \n",
    "<br>\n",
    "<br>\n",
    "**4.** Finally, we load the dataset. For this example, we'll be using `\"glue/mrpc\"` ([the Microsoft Research Paraphrase Corpus](https://www.microsoft.com/en-us/download/details.aspx?id=52398)), which we'll instantiate using the `tensorflow_datasets` package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Overwrite dataset info from restored data version.\n",
      "INFO:absl:Reusing dataset glue (/home/mikekane00/tensorflow_datasets/glue/mrpc/1.0.0)\n",
      "INFO:absl:Constructing tf.data.Dataset for split None, from /home/mikekane00/tensorflow_datasets/glue/mrpc/1.0.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import * \n",
    "import tensorflow_datasets\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-cased')\n",
    "dataset = tensorflow_datasets.load('glue/mrpc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use some preprocessing tools from the `transformers` package to prepare our dataset. Note that the tools work seamlessly with a TensorFlow `Dataset` object.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = glue_convert_examples_to_features(dataset['train'], tokenizer, max_length=128, task='mrpc')\n",
    "valid_dataset = glue_convert_examples_to_features(dataset['validation'], tokenizer, max_length=128, task='mrpc')\n",
    "train_dataset = train_dataset.shuffle(100).batch(32).repeat(2)\n",
    "valid_dataset = valid_dataset.batch(64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next part is pure Tensorflow. We compile and train our model, using Keras syntax. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',  loss='sparse_categorical_crossentropy', metrics=['sparse_categorical_crossentropy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 115 steps, validate for 7 steps\n",
      "Epoch 1/2\n",
      "115/115 [==============================] - 2418s 21s/step - loss: 0.7224 - sparse_categorical_crossentropy: 0.7225 - val_loss: 0.6931 - val_sparse_categorical_crossentropy: 0.6931\n",
      "Epoch 2/2\n",
      "115/115 [==============================] - 2373s 21s/step - loss: 0.6930 - sparse_categorical_crossentropy: 0.6930 - val_loss: 0.6931 - val_sparse_categorical_crossentropy: 0.6931\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_dataset, epochs=2, steps_per_epoch=115, validation_data=valid_dataset, validation_steps=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily save our trained model.  Note that the save file is library-agnostic. We can easily load it up in a PyTorch model. We'll reload our saved model as a PyTorch model, and then use it for inference on a few tasks in order to check that it works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('./save/')\n",
    "pytorch_model = BertForSequenceClassification.from_pretrained('./save/', from_tf=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final cell block is standard pytorch. We'll use it for inference. Note that PyTorch models have two separate modes--training, and evaluation. When models are created, they are in train mode by default. However, when loaded as a pretrained model, they are loaded in evaluation mode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence_1 is not a paraphrase of sentence_0\n",
      "sentence_2 is not a paraphrase of sentence_0\n"
     ]
    }
   ],
   "source": [
    "sentence_0 = \"Mark and his friends went to the movies.\"\n",
    "sentence_1 = \"Mark and Tim went to a movie.\"\n",
    "sentence_2 = \"His findings were not compatible with this research.\"\n",
    "inputs_1 = tokenizer.encode_plus(sentence_0, sentence_1, add_special_tokens=True, return_tensors='pt')\n",
    "inputs_2 = tokenizer.encode_plus(sentence_0, sentence_2, add_special_tokens=True, return_tensors='pt')\n",
    "\n",
    "pred_1 = pytorch_model(inputs_1['input_ids'], token_type_ids=inputs_1['token_type_ids'])[0].argmax().item()\n",
    "pred_2 = pytorch_model(inputs_2['input_ids'], token_type_ids=inputs_2['token_type_ids'])[0].argmax().item()\n",
    "\n",
    "print(\"sentence_1 is\", \"a paraphrase\" if pred_1 else \"not a paraphrase\", \"of sentence_0\")\n",
    "print(\"sentence_2 is\", \"a paraphrase\" if pred_2 else \"not a paraphrase\", \"of sentence_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
